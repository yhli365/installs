Spark
@version spark-1.2.0-bin-hadoop2.4.tgz

#)安装(所有节点)

#)配置(所有节点)
$ cp $SPARK_HOME/conf/slaves.template $SPARK_HOME/conf/slaves
$ vi $SPARK_HOME/conf/slaves
devs1
devs2
devs3

$ cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh
$ vi $SPARK_HOME/conf/spark-env.sh
SPARK_MASTER_IP=devs1
SPARK_MASTER_PORT=7077
SPARK_WORKER_CORES=1
SPARK_WORKER_MEMORY=2g
SPARK_WORKER_INSTANCES=1

#)启动
$ cd $SPARK_HOME
$ sbin/start-all.sh
$ sbin/stop-all.sh

MasterWebUI at http://devs1:8080

$ bin/spark-shell \
--driver-memory 512m \
--executor-memory 512m \
--master spark://devs1:7077

scala> exit
val textFile = sc.textFile("README.md")
textFile.count()
textFile.first()
textFile.filter(line => line.contains("Spark")).count()

textFile.map(line => line.split(" ").size).reduce((a, b) => if (a > b) a else b)
val wordCounts = textFile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey((a, b) => a + b)
wordCounts.collect()

val linesWithSpark = textFile.filter(line => line.contains("Spark"))
linesWithSpark.count()
linesWithSpark.cache()
linesWithSpark.count()

$ bin/pyspark --driver-memory 512m --executor-memory 512m --master spark://devs1:7077
>>> exit();
textFile = sc.textFile("README.md")
textFile.count()
textFile.first()
textFile.filter(lambda line: "Spark" in line).count()

textFile.map(lambda line: len(line.split())).reduce(lambda a, b: a if (a > b) else b)
wordCounts = textFile.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)
wordCounts.collect()

linesWithSpark = textFile.filter(lambda line: "Spark" in line)
linesWithSpark.count()
linesWithSpark.cache()
linesWithSpark.count()

