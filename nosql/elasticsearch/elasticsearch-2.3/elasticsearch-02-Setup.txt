Elasticsearch Reference
=============================
https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html


Setup
=============================
# Supported platforms

# Installation
$ bin/elasticsearch

# Running as a daemon
$ bin/elasticsearch -d

# PID
$ bin/elasticsearch -d -p pid
$ kill `cat pid`

# Another feature is the ability to pass -D or getopt long style configuration parameters directly to the script. When set, all override anything set using either JAVA_OPTS or ES_JAVA_OPTS.
$ bin/elasticsearch -Des.index.refresh_interval=5s --node.name=my-node

# Java (JVM) version
# We recommend installing the Java 8 update 20 or later, or Java 7 update 55 or later.


Configuration
=============================
--Environment Variables 
# JAVA_OPTS ES_JAVA_OPTS ES_HEAP_SIZE
# It is recommended to set the min and max memory to the same value, and enable mlockall.

--System Configuration
# File Descriptors
# Make sure to increase the number of open files descriptors on the machine (or for the user running elasticsearch). Setting it to 32k or even 64k is recommended.
curl localhost:9200/_nodes/stats/process?pretty

# Virtual memory
# Elasticsearch uses a hybrid mmapfs / niofs directory by default to store its indices.
$ sysctl vm.max_map_count
$ sysctl -w vm.max_map_count=262144
# To set this value permanently, update the vm.max_map_count setting in /etc/sysctl.conf.

# Memory Settings
# Swapping is very bad for performance and for node stability, so it should be avoided at all costs.
# There are three options:
# -Disable swap
# On Linux systems, you can disable swap temporarily by running: sudo swapoff -a. To disable it permanently, you will need to edit the /etc/fstab file and comment out any lines that contain the word swap.
# -Configure swappiness
# The second option is to ensure that the sysctl value vm.swappiness is set to 0. This reduces the kernelâ€™s tendency to swap and should not lead to swapping under normal circumstances, while still allowing the whole system to swap in emergency conditions.
# -mlockall
# The third option is to use mlockall on Linux/Unix systems, or VirtualLock on Windows, to try to lock the process address space into RAM, preventing any Elasticsearch memory from being swapped out. This can be done, by adding this line to the config/elasticsearch.yml file:
bootstrap.mlockall: true
curl http://localhost:9200/_nodes/process?pretty

--Elasticsearch Settings
# The configuration format is YAML. 
$ vi config/elasticsearch.yml
network :
    host : 10.0.0.4

# Paths
# In production use, you will almost certainly want to change paths for data and log files:
path:
  logs: /var/log/elasticsearch
  data: /var/data/elasticsearch

# Cluster name
cluster:
  name: <NAME OF YOUR CLUSTER>

# Node name
node:
  name: <NAME OF YOUR NODE>
# If on your machine you only run a single elasticsearch node for that cluster
node:
  name: ${HOSTNAME}

--Configuration styles
# If JSON is a preferred configuration format, simply rename the elasticsearch.yml file to elasticsearch.json and add:
{
    "network" : {
        "host" : "10.0.0.4"
    }
}

--Index Settings
# Indices created within the cluster can provide their own settings. 
# For example, the following creates an index with a refresh interval of 5 seconds instead of the default refresh interval (the format can be either YAML or JSON):
$ curl -XPUT http://localhost:9200/kimchy/ -d \
'
index:
    refresh_interval: 5s
'

# Index level settings can be set on the node level as well, for example, within the elasticsearch.yml file
index :
    refresh_interval: 5s

# Of course, the above can also be set as a "collapsed" setting, for example:
$ elasticsearch -Des.index.refresh_interval=5s

--Logging
# Elasticsearch uses an internal logging abstraction and comes, out of the box, with log4j. 
$ vi config/logging.yml


Running as a Service on Linux
=============================
# RPM package


Running as a Service on Windows
=============================
c:\elasticsearch-2.3.4\bin> service
Usage: service.bat install|remove|start|stop|manager [SERVICE_ID]

cmd> service install es01
cmd> service manager es01
cmd> service remove es01

cmd> service start es01
cmd> service stop es01


Directory Layout
=============================
# Location RHEL/CentOS for RPM
home	/usr/share/elasticsearch
bin		/usr/share/elasticsearch/bin
conf	/etc/elasticsearch
conf	/etc/sysconfig/elasticsearch
data	/var/lib/elasticsearch
logs	/var/log/elasticsearch
plugins	/usr/share/elasticsearch/plugins
repo	Not configured
script	/etc/elasticsearch/scripts


Repositories
=============================
# We also have repositories available for APT and YUM based distributions. 

--YUM / DNF
# Download and install the public signing key:
$ rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
$ vi /etc/yum.repos.d/elasticsearch.repo
[elasticsearch-2.x]
name=Elasticsearch repository for 2.x packages
baseurl=https://packages.elastic.co/elasticsearch/2.x/centos
gpgcheck=1
gpgkey=https://packages.elastic.co/GPG-KEY-elasticsearch
enabled=1
$ yum install elasticsearch
$ chkconfig --add elasticsearch


Upgrading
=============================
# Supported Upgrade Type
Full cluster restart
Rolling upgrade

# Take plugins into consideration as well when upgrading. Plugins must be upgraded alongside Elasticsearch.

# The process for both Rolling upgrade and Full cluster restart is generally as follows, per node.
    Shut down Elasticsearch
    Upgrade Elasticsearch
    Upgrade Plugins
    Start up Elasticsearch 


--Back Up Your Data!
# Always back up your data before performing an upgrade. This will allow you to roll back in the event of a problem.

# Backing up 1.0 and later
To back up a running 1.0 or later system, it is simplest to use the snapshot feature. 

--Rolling upgrades
Step 1: Disable shard allocation
Step 2: Stop non-essential indexing and perform a synced flush (Optional)
Step 3: Stop and upgrade a single node
Step 4: Start the upgraded node
Step 5: Reenable shard allocation
Step 6: Wait for the node to recover
Step 7: Repeat

--Full cluster restart upgrade
Step 1: Disable shard allocation
Step 2: Perform a synced flush
Step 3: Shutdown and upgrade all nodes
Step 4: Start the cluster
Step 5: Wait for yellow
Step 6: Reenable allocation

