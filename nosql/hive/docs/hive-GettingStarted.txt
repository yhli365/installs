Hive GettingStarted
@site https://cwiki.apache.org/confluence/display/Hive/GettingStarted
@version hive-0.13.0

--)Hive, Map-Reduce and Local-Mode
hive> SET mapred.job.tracker=local;
mapred.local.dir=/tmp/yhli/mapred/local

hive> SET hive.exec.mode.local.auto=false;
hive.exec.mode.local.auto.inputbytes.max=128MB
hive.exec.mode.local.auto.tasks.max=4
hive.mapred.local.mem=0

--)DDL Operations
hive> CREATE TABLE pokes (foo INT, bar STRING);
hive> CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING);
hive> SHOW TABLES;
hive> SHOW TABLES '.*s';
hive> DESCRIBE invites;

hive> ALTER TABLE invites ADD COLUMNS (new_col2 INT COMMENT 'a comment');
hive> DESCRIBE invites;
hive> ALTER TABLE invites REPLACE COLUMNS (foo INT, bar STRING, baz INT COMMENT 'baz replaces new_col2');
hive> DESCRIBE invites;
hive> ALTER TABLE invites REPLACE COLUMNS (foo INT COMMENT 'only keep the first column');
hive> DESCRIBE invites;

hive> SHOW TABLES;
hive> ALTER TABLE pokes RENAME TO events;
hive> SHOW TABLES;
hive> DROP TABLE events;
hive> SHOW TABLES;
hive> DROP TABLE invites;

#Metadata Store
javax.jdo.option.ConnectionURL=./metastore_db
javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.EmbeddedDriver
hive.metastore.warehouse.dir=/user/hive/warehouse

--)DML Operations
[yhli@vm1 apache-hive-0.13.0-bin]$ hadoop fs -put ./examples/files/kv2.txt /user/yhli/hive/kv2.txt
[yhli@vm1 apache-hive-0.13.0-bin]$ hive
hive> CREATE TABLE pokes (foo INT, bar STRING);
hive> CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING);

hive> LOAD DATA LOCAL INPATH './examples/files/kv1.txt' OVERWRITE INTO TABLE pokes;
hive> LOAD DATA INPATH '/user/yhli/hive/kv2.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-15');
hive> LOAD DATA LOCAL INPATH './examples/files/kv3.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-08');

--)SQL Operations
#SELECTS and FILTERS
hive> SELECT a.foo FROM invites a WHERE a.ds='2008-08-15';
hive> INSERT OVERWRITE DIRECTORY '/tmp/hdfs_out' SELECT a.* FROM invites a WHERE a.ds='2008-08-15';
hive> INSERT OVERWRITE LOCAL DIRECTORY '/tmp/local_out' SELECT a.* FROM pokes a;

hive> INSERT OVERWRITE TABLE events SELECT a.* FROM profiles a;
hive> INSERT OVERWRITE TABLE events SELECT a.* FROM profiles a WHERE a.key < 100;
hive> INSERT OVERWRITE LOCAL DIRECTORY '/tmp/reg_3' SELECT a.* FROM events a;
hive> INSERT OVERWRITE DIRECTORY '/tmp/reg_4' select a.invites, a.pokes FROM profiles a;
hive> INSERT OVERWRITE DIRECTORY '/tmp/reg_5' SELECT COUNT(*) FROM invites a WHERE a.ds='2008-08-15';
hive> INSERT OVERWRITE DIRECTORY '/tmp/reg_6' SELECT a.foo, a.bar FROM invites a;
hive> INSERT OVERWRITE LOCAL DIRECTORY '/tmp/sum' SELECT SUM(a.pc) FROM pc1 a;

#GROUP BY
hive> CREATE TABLE events (name STRING, cnt INT);
hive> FROM invites a INSERT OVERWRITE TABLE events SELECT a.bar, count(*) WHERE a.foo > 0 GROUP BY a.bar;
hive> select * from events limit 20;
hive> INSERT OVERWRITE TABLE events SELECT a.bar, count(*) FROM invites a WHERE a.foo > 0 GROUP BY a.bar;

#JOIN
hive> CREATE TABLE events (bar STRING, foo1 INT, foo2 INT);
hive> FROM pokes t1 JOIN invites t2 ON (t1.bar = t2.bar) INSERT OVERWRITE TABLE events SELECT t1.bar, t1.foo, t2.foo;
hive> select * from events limit 20;

#MULTITABLE INSERT
FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.* WHERE src.key < 100
INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key >= 100 and src.key < 200
INSERT OVERWRITE TABLE dest3 PARTITION(ds='2008-04-08', hr='12') SELECT src.key WHERE src.key >= 200 and src.key < 300
INSERT OVERWRITE LOCAL DIRECTORY '/tmp/dest4.out' SELECT src.value WHERE src.key >= 300;

#STREAMING
hive> FROM invites a INSERT OVERWRITE TABLE events SELECT TRANSFORM(a.foo, a.bar) AS (oof, rab) USING '/bin/cat' WHERE a.ds > '2008-08-09';

--)Simple Example Use Cases
1)MovieLens User Ratings
[yhli@vm1 apache-hive-0.13.0-bin]$ cd ~/data/hive/MovieLens/
[yhli@vm1 MovieLens]$ hive

#First, create a table with tab-delimited text file format:
CREATE TABLE u_data (
  userid INT,
  movieid INT,
  rating INT,
  unixtime STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

#Then, download and extract the data files:
wget http://www.grouplens.org/sites/www.grouplens.org/external_files/data/ml-data.tar.gz
tar xvzf ml-data.tar.gz

#And load it into the table that was just created:
LOAD DATA LOCAL INPATH 'ml-data/u.data'
OVERWRITE INTO TABLE u_data;

#Count the number of rows in table u_data:
SELECT COUNT(*) FROM u_data;
SELECT * FROM u_data LIMIT 20;

#Now we can do some complex data analysis on the table u_data:
#Create weekday_mapper.py:
[yhli@vm1 MovieLens]$ vi weekday_mapper.py
import sys
import datetime

for line in sys.stdin:
  line = line.strip()
  userid, movieid, rating, unixtime = line.split('\t')
  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()
  print '\t'.join([userid, movieid, rating, str(weekday)])

[yhli@vm1 MovieLens]$ echo -e "196\t242\t3\t881250949" | python weekday_mapper.py
196     242     3       4

#Use the mapper script:
CREATE TABLE u_data_new (
  userid INT,
  movieid INT,
  rating INT,
  weekday INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t';

add FILE weekday_mapper.py;

INSERT OVERWRITE TABLE u_data_new
SELECT
  TRANSFORM (userid, movieid, rating, unixtime)
  USING 'python weekday_mapper.py'
  AS (userid, movieid, rating, weekday)
FROM u_data;

SELECT weekday, COUNT(*)
FROM u_data_new
GROUP BY weekday;

#My test:
SELECT * FROM u_data_new LIMIT 20;

SELECT weekday, COUNT(*) as cnt
FROM u_data_new
GROUP BY weekday
ORDER BY cnt DESC;

2)Apache Weblog Data
CREATE TABLE apachelog (
  host STRING,
  identity STRING,
  user STRING,
  time STRING,
  request STRING,
  status STRING,
  size STRING,
  referer STRING,
  agent STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "([^]*) ([^]*) ([^]*) (-|\\[^\\]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\".*\") ([^ \"]*|\".*\"))?"
)
STORED AS TEXTFILE;

